temperature:
  type: float
  range: [0.0, 2.0]
  default: 1.0
  explainer_url: "https://youtu.be/ezgqHnWvua8"
  description: >
    Controls response variety. Lower = more predictable; higher = more diverse.
    At 0, the model gives the same response for a given input.
top_p:
  type: float
  range: [0.0, 1.0]
  default: 1.0
  explainer_url: "https://youtu.be/wQP-im_HInk"
  description: >
    This setting limits the model’s choices to a percentage of likely tokens: 
    only the top tokens whose probabilities add up to P. A lower value makes the model’s 
    responses more predictable, while the default setting allows for a full range of 
    token choices. Think of it like a dynamic Top-K.
top_k:
  type: int
  range: [0]
  default: 0
  explainer_url: "https://youtu.be/EbZv6-N8Xlk"
  description: >
    This limits the model’s choice of tokens at each step, making it choose from a smaller set. 
    A value of 1 means the model will always pick the most likely next token, leading to 
    predictable results. By default this setting is disabled, making the model to consider all 
    choices.
frequency_penalty:
  type: float
  range: [-2.0, 2.0]
  default: 0.0
  explainer_url: "https://youtu.be/p4gl6fqI0_w"
  description: >
    This setting aims to control the repetition of tokens based on how often they appear in the input. 
    It tries to use less frequently those tokens that appear more in the input, proportional to how 
    frequently they occur. Token penalty scales with the number of occurrences. Negative values will 
    encourage token reuse.
presence_penalty:
  type: float
  range: [-2.0, 2.0]
  default: 0.0
  explainer_url: "https://youtu.be/MwHG5HL-P74"
  description: >
    Adjusts how often the model repeats specific tokens already used in the input. Higher values make 
    such repetition less likely, while negative values do the opposite. Token penalty does not scale 
    with the number of occurrences. Negative values will encourage token reuse.
repetition_penalty:
  type: float
  range: [0.0, 2.0]
  default: 1.0
  explainer_url: "https://youtu.be/LHjGAnLm3DM"
  description: >
    Helps to reduce the repetition of tokens from the input. A higher value makes the model less 
    likely to repeat tokens, but too high a value can make the output less coherent (often with 
    run-on sentences that lack small words). Token penalty scales based on original token’s probability.
min_p:
  type: float
  range: [0.0, 1.0]
  default: 0.0
  description: >
    Represents the minimum probability for a token to be considered, relative to the probability 
    of the most likely token. (The value changes depending on the confidence level of the most 
    probable token.) If your Min-P is set to 0.1, that means it will only allow for tokens that 
    are at least 1/10th as probable as the best possible option.
top_a:
  type: float
  range: [0.0, 1.0]
  default: 0.0
  description: >
    Consider only the top tokens with “sufficiently high” probabilities based on the probability
    of the most likely token. Think of it like a dynamic Top-P. A lower Top-A value focuses the 
    choices based on the highest probability token but with a narrower scope. A higher Top-A 
    value does not necessarily affect the creativity of the output, but rather refines the 
    filtering process based on the maximum probability.
seed:
  type: boolean
  default: False
  description: >
    If specified, the inferencing will sample deterministically, such that repeated requests 
    with the same seed and parameters should return the same result. Determinism is not 
    guaranteed for some models.
max_tokens:
  type: int
  range: [1]
  description: >
    This sets the upper limit for the number of tokens the model can generate in response. It won’t 
    produce more than this limit. The maximum value is the context length minus the prompt length.
verbosity:
  type: enum
  options: ["low", "medium", "high"]
  default: "medium"
  description: >
    Controls the verbosity and length of the model response. Lower values produce more concise 
    responses, while higher values produce more detailed and comprehensive responses.